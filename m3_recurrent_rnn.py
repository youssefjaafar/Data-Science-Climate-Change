# -*- coding: utf-8 -*-
"""m3_recurrent_rnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XheW4Fvr9P5P2pEM9FQ2uIBOz7JDjUft
"""

from google.colab import drive
drive.mount("/content/drive")

#Load Libraries
import pandas 
import numpy 
import matplotlib.pyplot as plt

"""#Neural Network

### Data Preparation
"""

#separate the other attributes from the predicting attribute
x_train = pandas.read_csv(r'/content/drive/My Drive/CMPS_276/final_data/X_train.csv')
#separte the predicting attribute into Y for model training 
y_train = pandas.read_csv(r'/content/drive/My Drive/CMPS_276/final_data/Y_train.csv')
features = ['whc', 'silt', 'silt', 'clay',	'om',	'kwfactor',	'kffactor',	'spH',
            'tfactor',	'Yearly Precipitation',	'Yearly Average Temperature','Value of Previous Year']
target = ['Value']
#x_train = x_train[features]
#y_train = y_train[target]

x_test = pandas.read_csv(r'/content/drive/My Drive/CMPS_276/final_data/X_test.csv')
#x_test = x_test[features]
y_test = pandas.read_csv(r'/content/drive/My Drive/CMPS_276/final_data/Y_test.csv')
#y_test=y_test[target]
# y_prediction =  LR.predict(numpy.nan_to_num(x_test))
# y_prediction

print(x_train.shape) 
print(y_train.shape)

x_train.info()

x_train= x_train.drop(columns=['Value of Previous Year', 'Unnamed: 0'])

x_test= x_test.drop(columns=['Value of Previous Year', 'Unnamed: 0'])

y_train= y_train.iloc[:,1:2]
y_train.head()
print(y_train.shape)

y_test= y_test.iloc[:,1:2]
#y_test.head()

print(f'We have {x_train.shape[0]} train samples')
print(f'We have {x_test.shape[0]} test samples')

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

y_train.head()

y_test.head()

import tensorflow as tf
import keras
from keras import layers
from keras import models
from keras import utils
from keras.layers import Dense
from keras.models import Sequential
from keras.layers import Flatten
from keras.layers import Dropout
from keras.layers import Activation
from keras.regularizers import l2
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers import RMSprop
from keras import datasets

from keras.callbacks import LearningRateScheduler
from keras.callbacks import History

from keras import losses
from sklearn.utils import shuffle

# fix random seed for reproducibility
numpy.random.seed(5)

"""### Model Trials """

epochs=60
learning_rate = 0.1
decay_rate = learning_rate / epochs
momentum = 0.8

sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)

# build the NN model

input_dim = x_train.shape[1]

lr_model = Sequential()
lr_model.add(Dense(1024, activation=tf.nn.relu, 
                input_dim = input_dim)) 
lr_model.add(Dropout(0.1))
lr_model.add(Dense(512, kernel_initializer='uniform', activation=tf.nn.relu))
lr_model.add(Dense(1, activation=tf.nn.relu))

# compile the model
lr_model.compile(loss='mse',
              optimizer=sgd, #rms
              metrics=['acc'])

# Fit the model
batch_size = int(input_dim/100)

lr_model_history = lr_model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(x_test, y_test))

# Plot the loss function
fig, ax = plt.subplots(1, 1, figsize=(10,6))
ax.plot(np.sqrt(lr_model_history.history['loss']), 'r', label='train')
ax.plot(np.sqrt(lr_model_history.history['val_loss']), 'b' ,label='val')
ax.set_xlabel(r'Epoch', fontsize=20)
ax.set_ylabel(r'Loss', fontsize=20)
ax.legend()
ax.tick_params(labelsize=20)

# Plot the accuracy
fig, ax = plt.subplots(1, 1, figsize=(10,6))
ax.plot(np.sqrt(lr_model_history.history['acc']), 'r', label='train')
ax.plot(np.sqrt(lr_model_history.history['val_acc']), 'b' ,label='val')
ax.set_xlabel(r'Epoch', fontsize=20)
ax.set_ylabel(r'Accuracy', fontsize=20)
ax.legend()
ax.tick_params(labelsize=20)

# hyperparam
import numpy
from sklearn.model_selection import GridSearchCV
from keras.wrappers.scikit_learn import KerasClassifier

# let's create a function that creates the model (required for KerasClassifier) 
# while accepting the hyperparameters we want to tune 
# we also pass some default values such as optimizer='rmsprop'
def create_model(init_mode='uniform'):
    # define model
    model = Sequential()
    model.add(Dense(64, kernel_initializer=init_mode, activation=tf.nn.relu, input_dim=31)) 
    model.add(Dropout(0.1))
    model.add(Dense(64, kernel_initializer=init_mode, activation=tf.nn.relu))
    model.add(Dense(10, kernel_initializer=init_mode, activation=tf.nn.softmax))
    # compile model
    model.compile(loss='mse',
              optimizer=RMSprop(),
              metrics=['accuracy'])
    return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# seed = 7
# numpy.random.seed(seed)
# batch_size = 128
# epochs = 10
# 
# model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, 
#                            batch_size=batch_size, verbose=1)
# # define the grid search parameters
# init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 
#              'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']
# 
# param_grid = dict(init_mode=init_mode)
# grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3)
# grid_result = grid.fit(x_train, y_train)

# print results
print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(f' mean={mean:.4}, std={stdev:.4} using {param}')

# repeat some of the initial values here so we make sure they were not changed
input_dim = x_train.shape[1]

# let's create a function that creates the model (required for KerasClassifier) 
# while accepting the hyperparameters we want to tune 
# we also pass some default values such as optimizer='rmsprop'
def create_model_2(optimizer='rmsprop', init='zero'):
    model = Sequential()
    model.add(Dense(64, input_dim=input_dim, kernel_initializer=init, activation='relu'))
    model.add(Dropout(0.1))
    model.add(Dense(64, kernel_initializer=init, activation=tf.nn.relu))
    model.add(Dense(1, kernel_initializer=init, activation=tf.nn.relu))

    # compile model
    model.compile(loss='mse', 
                  optimizer=optimizer, 
                  metrics=['accuracy'])

    return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # fix random seed for reproducibility (this might work or might not work 
# # depending on each library's implenentation)
# seed = 7
# numpy.random.seed(seed)
# 
# # create the sklearn model for the network
# model_init_batch_epoch_CV = KerasClassifier(build_fn=create_model_2, verbose=1)
# 
# # we choose the initializers that came at the top in our previous cross-validation!!
# init_mode = ['glorot_uniform', 'uniform'] 
# batches = [128, 512]
# epochs = [10, 20]
# 
# # grid search for initializer, batch size and number of epochs
# param_grid = dict(epochs=epochs, batch_size=batches, init=init_mode)
# grid = GridSearchCV(estimator=model_init_batch_epoch_CV, 
#                     param_grid=param_grid,
#                     cv=3)
# grid_result = grid.fit(x_train, y_train)

# print results
print(f'Best Accuracy for {grid_result.best_score_:.4} using {grid_result.best_params_}')
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(f'mean={mean:.4}, std={stdev:.4} using {param}')

"""https://towardsdatascience.com/simple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594

### Final Models Used for Evaluation

#### Model 1 Basic FNN
"""

# hidden_nodes=64

# model = Sequential()
# #model.add(LSTM(hidden_nodes, return_sequences=False, input_dim=input_dim))
# model.add(Dropout(0.2))
# model.add(Dense(1, kernel_initializer=init, activation=tf.nn.sigmoid))

from keras.models import Sequential
from keras.layers import Dense

model = Sequential([
    Dense(32, activation='relu', input_shape=(30,)),
    Dense(32, activation='relu'),
    Dense(1, activation='relu'),
])

# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])

model.compile(optimizer='sgd',
              loss='mse',
              metrics=['mean_squared_error'])

batch_size=1000
model.fit(x_train, y_train, batch_size=batch_size, epochs=10, validation_data=(x_test, y_test))

import matplotlib.pyplot as plt

hist = model.fit(x_train, y_train,
          batch_size=32, epochs=100,
          validation_split=0.2)

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
plt.show()

y_pred= model.predict(x_test)

import sklearn.metrics as metrics

mae = metrics.mean_absolute_error(y_test, y_pred)
mse = metrics.mean_squared_error(y_test, y_pred)
rmse = numpy.sqrt(mse) # or mse**(0.5)  
r2 = metrics.r2_score(y_test, y_pred)

# print("Results of sklearn.metrics:")
print("MAE:",mae)
print("MSE:", mse)
print("RMSE:", rmse)
print("R-Squared:", r2)

print(y_pred)

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
# plt.ylim(top=1.2, bottom=0)
plt.show()

plt.plot(hist.history['mean_squared_error'])
plt.plot(hist.history['val_mean_squared_error'])
plt.title('Model Mean Squared Error')
plt.ylabel('Mean Squared Error')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()

"""#### Model 1 with Dropout and L2 Penalty"""

from keras.layers import Dropout
from keras import regularizers

model_2 = Sequential([
    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(30,)),
    Dropout(0.3),
    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.3),
    # Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    # Dropout(0.3),
    # Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    # Dropout(0.3),
    Dense(1, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
])

# To add L2 regularization, notice that we’ve added a bit of extra code in each of our dense layers like this: kernel_regularizer=regularizers.l2(0.01)
# This tells Keras to include the squared values of those parameters in our overall loss function, and weight them by 0.01 in the loss function.
# To add Dropout, we added a new layer like this: Dropout(0.3),
# This means that the neurons in the previous layer has a probability of 0.3 in dropping out during training. Let’s compile it and run it with the same parameters as our Model 2 (the overfitting one):

sgd = SGD(lr=0.001, momentum=0.9)
model_2.compile(loss='mse', optimizer=sgd, metrics=['mean_squared_error'])

model_2.summary()

# fit model
# history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)

# model_2.compile(optimizer='sgd',
#               loss='mse', 
#               metrics=['mean_squared_error']
#               )
              
hist_2 = model_2.fit(x_train, y_train,
          batch_size=32, 
          epochs=100,
          #epochs=100,
          #validation_split=0.2
          validation_data=(x_test, y_test)
          )

y_pred_2= model_2.predict(x_test)

import sklearn.metrics as metrics

mae_2 = metrics.mean_absolute_error(y_test, y_pred_2)
mse_2 = metrics.mean_squared_error(y_test, y_pred_2)
rmse_2 = numpy.sqrt(mse_2) # or mse**(0.5)  
r2_2 = metrics.r2_score(y_test, y_pred_2)

# print("Results of sklearn.metrics:")
print("MAE:",mae_2)
print("MSE:", mse_2)
print("RMSE:", rmse_2)
print("R-Squared:", r2_2)

hist_2.history.keys()
# hist_2.history

plt.plot(hist_2.history['loss'])
plt.plot(hist_2.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
# plt.ylim(top=1.2, bottom=0)
plt.show()

plt.plot(hist_2.history['mean_squared_error'])
plt.plot(hist_2.history['val_mean_squared_error'])
plt.title('Model Mean Squared Error')
plt.ylabel('Mean Squared Error')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()

"""#### Hyperparameter Tuning

##### Tuning Number of Neurons
"""

# Use scikit-learn to grid search the number of neurons
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasRegressor
from keras.constraints import maxnorm

# Function to create model, required for KerasClassifier
def create_model(neurons=32):
	# create model
	model = Sequential()
	model.add(Dense(neurons, input_dim=30, kernel_initializer='uniform', activation='relu'))
	model.add(Dropout(0.2))
	model.add(Dense(1, kernel_initializer='uniform', activation='relu'))
	# Compile model
	model.compile(loss='mse', optimizer='sgd', metrics=['mse'])
	return model

# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)

# create model
model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=10, verbose=1)

# define the grid search parameters
neurons = [10, 32, 64, 128]
param_grid = dict(neurons=neurons)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=2)
grid_result = grid.fit(x_train, y_train)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# hyperparam
import numpy
from sklearn.model_selection import GridSearchCV
from keras.wrappers.scikit_learn import KerasClassifier

def create_model():
    # define model
    model = Sequential()
    model.add(Dense(64, activation=tf.nn.relu, input_dim=(32,)) 
    model.add(Dense(64, activation=tf.nn.relu))
    model.add(Dense(1, activation=tf.nn.relu))
    # compile model
    model.compile(loss='mse',
              optimizer=RMSprop(),
              metrics=['accuracy'])
    return model

"""##### Tuning Batch Size and Number of Epochs"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # fix random seed for reproducibility (this might work or might not work 
# # depending on each library's implenentation)
# seed = 7
# numpy.random.seed(seed)
# 
# # create the sklearn model for the network
# model_init_batch_epoch_CV = KerasClassifier(build_fn=create_model_2, verbose=1)
# 
# # we choose the initializers that came at the top in our previous cross-validation!!
# # init_mode = ['glorot_uniform', 'uniform'] 
# batches = [10, 100, 150, 200, 250]
# epochs = [32, 64, 128]
# 
# # grid search for initializer, batch size and number of epochs
# param_grid = dict(epochs=epochs, batch_size=batches, init=init_mode)
# grid = GridSearchCV(estimator=model_init_batch_epoch_CV, 
#                     param_grid=param_grid,
#                     cv=3)
# grid_result = grid.fit(x_train, y_train)

"""https://www.freecodecamp.org/news/how-to-build-your-first-neural-network-to-predict-house-prices-with-keras-f8db83049159/"""

model = Sequential([
    Dense(64, activation='relu', input_shape=(30,)),
    Dense(64, activation='relu'),
    Dense(1, activation='relu'),
])

# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])

model.compile(optimizer='sgd',
              loss='mse',
              metrics=['mean_squared_error'])

batch_size=300
hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=32, validation_data=(x_test, y_test))

import matplotlib.pyplot as plt

# hist = model.fit(x_train, y_train,
#           batch_size=32, epochs=300,
#           validation_split=0.2)

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
plt.show()

y_pred= model.predict(x_test)

import sklearn.metrics as metrics

mae = metrics.mean_absolute_error(y_test, y_pred)
mse = metrics.mean_squared_error(y_test, y_pred)
rmse = numpy.sqrt(mse) # or mse**(0.5)  
r2 = metrics.r2_score(y_test, y_pred)

# print("Results of sklearn.metrics:")
print("MAE:",mae)
print("MSE:", mse)
print("RMSE:", rmse)
print("R-Squared:", r2)

plt.plot(hist.history['mean_squared_error'])
plt.plot(hist.history['val_mean_squared_error'])
plt.title('Model Mean Squared Error')
plt.ylabel('Mean Squared Error')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()

print(y_pred)

"""#### Model using Selected Features Only (and optimal hyperparameters)"""